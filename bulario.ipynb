{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2MnT8+cNux2O9o+4j429P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luasampaio/CienciaDados/blob/main/bulario.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-WCl0lp_GGy",
        "outputId": "effec1ab-b1fa-4732-dc5b-d71fb503b786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download completo: bulario.html\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "# URL do arquivo a ser baixado\n",
        "url = \"https://www.dormaissaude.com.br/bulario\"\n",
        "\n",
        "# Realiza a requisição GET para a URL\n",
        "response = requests.get(url, allow_redirects=True)\n",
        "\n",
        "# Verifica se a requisição foi bem-sucedida\n",
        "if response.status_code == 200:\n",
        "    # Define o nome do arquivo onde o conteúdo será salvo\n",
        "    filename = \"bulario.html\"  # ou .pdf ou outro formato dependendo do que você deseja baixar\n",
        "\n",
        "    # Salva o conteúdo do arquivo\n",
        "    with open(filename, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "\n",
        "    print(f\"Download completo: {filename}\")\n",
        "else:\n",
        "    print(f\"Falha no download. Código de status: {response.status_code}\")\n"
      ]
    },
    {
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install pandas\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# URL do site\n",
        "url = \"https://www.dormaissaude.com.br/bulario\"\n",
        "\n",
        "# Realiza a requisição GET\n",
        "response = requests.get(url)\n",
        "\n",
        "# Verifica se a requisição foi bem-sucedida\n",
        "if response.status_code == 200:\n",
        "  # Analisa o conteúdo HTML com BeautifulSoup\n",
        "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "  # **Verifique se a tabela existe e ajuste o seletor se necessário**\n",
        "\n",
        "  table = soup.find('table', {'class': 'bulario-tabela'})\n",
        "\n",
        "\n",
        "  if table:\n",
        "    # Extrai os dados da tabela\n",
        "    data = []\n",
        "    for row in table.find_all('tr'):\n",
        "      cols = row.find_all('td')\n",
        "      cols = [ele.text.strip() for ele in cols]\n",
        "      data.append([ele for ele in cols if ele])\n",
        "\n",
        "    # Cria um DataFrame pandas com os dados\n",
        "    df = pd.DataFrame(data[1:], columns=data[0])\n",
        "\n",
        "    # Salva o DataFrame em um arquivo CSV\n",
        "    df.to_csv('bulario.csv', index=False)\n",
        "\n",
        "    print(\"Bulario baixado com sucesso em bulario.csv\")\n",
        "  else:\n",
        "    print(\"Tabela não encontrada na página.\")\n",
        "else:\n",
        "  print(f\"Falha na requisição. Código de status: {response.status_code}\")\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4Pz7LSRAbN0",
        "outputId": "d48d1e03-17d9-461a-a47e-370f04d83f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Tabela não encontrada na página.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# URL base da API do Bulário Eletrônico da Anvisa\n",
        "url = \"https://consultas.anvisa.gov.br/api/v1/medicamentos/bulario\"\n",
        "\n",
        "# Parâmetros da requisição (ajuste conforme necessário)\n",
        "params = {\n",
        "    \"nome\": \"Dipirona\",  # Exemplo de busca por nome de medicamento\n",
        "    \"pagina\": 1,  # Paginação, caso haja mais de uma página de resultados\n",
        "}\n",
        "\n",
        "# Realiza a requisição GET\n",
        "response = requests.get(url, params=params)\n",
        "\n",
        "# Verifica se a requisição foi bem-sucedida\n",
        "if response.status_code == 200:\n",
        "    # Converte a resposta para JSON\n",
        "    data = response.json()\n",
        "\n",
        "    # Verifica se há dados no JSON\n",
        "    if data:\n",
        "        # Converte os dados JSON em um DataFrame pandas\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        # Salva o DataFrame em um arquivo CSV\n",
        "        df.to_csv('bulario_anvisa.csv', index=False)\n",
        "\n",
        "        print(\"Dados do Bulário baixados com sucesso em bulario_anvisa.csv\")\n",
        "    else:\n",
        "        print(\"Nenhum dado foi encontrado na resposta da API.\")\n",
        "else:\n",
        "    print(f\"Falha na requisição. Código de status: {response.status_code}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLGvRkFsCywN",
        "outputId": "7ed7dffe-5522-482a-d565-f26bd52306c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Falha na requisição. Código de status: 404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL base do Bulário Eletrônico da Anvisa\n",
        "url = \"https://consultas.anvisa.gov.br/#/bulario/\"\n",
        "\n",
        "# Realiza a requisição GET\n",
        "response = requests.get(url)\n",
        "\n",
        "# Verifica se a requisição foi bem-sucedida\n",
        "if response.status_code == 200:\n",
        "    # Analisa o conteúdo HTML com BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    # Aqui você deve identificar a tabela ou o elemento HTML que contém as informações desejadas\n",
        "    # Exemplo: encontrar todas as linhas de uma tabela específica\n",
        "    table = soup.find(\"table\")  # Ajuste conforme a estrutura HTML do site\n",
        "    rows = table.find_all(\"tr\") if table else []\n",
        "\n",
        "    data = []\n",
        "    for row in rows:\n",
        "        cols = row.find_all(\"td\")\n",
        "        cols = [ele.text.strip() for ele in cols]\n",
        "        data.append(cols)\n",
        "\n",
        "    # Converte a lista em um DataFrame pandas\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Salva os dados em um arquivo CSV\n",
        "    df.to_csv('bulario_anvisa.csv', index=False)\n",
        "\n",
        "    print(\"Dados do Bulário baixados com sucesso em bulario_anvisa.csv\")\n",
        "else:\n",
        "    print(f\"Falha na requisição. Código de status: {response.status_code}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn1NkWkhEBO3",
        "outputId": "8391e8fd-cfc4-47f3-8df3-b47aedf92581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dados do Bulário baixados com sucesso em bulario_anvisa.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL da página de pesquisa de bulas\n",
        "url = \"https://bula.gratis/\"\n",
        "\n",
        "# Parâmetros de busca (ajuste conforme necessário)\n",
        "params = {\n",
        "    \"busca\": \"Dipirona\"  # Nome do medicamento\n",
        "}\n",
        "\n",
        "# Realiza a requisição GET\n",
        "response = requests.get(url, params=params)\n",
        "\n",
        "# Verifica se a requisição foi bem-sucedida\n",
        "if response.status_code == 200:\n",
        "    # Analisa o conteúdo HTML com BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    # Encontre e extraia os dados da página\n",
        "    # Ajuste o seletor para encontrar a estrutura correta dos dados\n",
        "    # Exemplo: buscar todos os links para bulas\n",
        "    bula_links = soup.find_all('a', class_='link-classe')  # Ajuste a classe para corresponder ao HTML do site\n",
        "\n",
        "    data = []\n",
        "    for link in bula_links:\n",
        "        title = link.get_text(strip=True)\n",
        "        href = link.get('href')\n",
        "        data.append({'Title': title, 'URL': href})\n",
        "\n",
        "    # Converte a lista de dados em um DataFrame pandas\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Salva o DataFrame em um arquivo CSV\n",
        "    df.to_csv('bulario_gratis.csv', index=False)\n",
        "\n",
        "    print(\"Dados do Bulario Grátis baixados com sucesso em bulario_gratis.csv\")\n",
        "else:\n",
        "    print(f\"Falha na requisição. Código de status: {response.status_code}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAU3S0fkGU67",
        "outputId": "b3ac009e-bf99-47c9-c094-d2fdbb12c350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dados do Bulario Grátis baixados com sucesso em bulario_gratis.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL base do site Consulta Remédios\n",
        "url = \"https://consultaremedios.com.br/\"\n",
        "\n",
        "# Realiza a requisição GET\n",
        "response = requests.get(url)\n",
        "\n",
        "# Verifica se a requisição foi bem-sucedida\n",
        "if response.status_code == 200:\n",
        "    # Analisa o conteúdo HTML com BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    # Encontre e extraia os dados da página\n",
        "    # Exemplo: buscar todos os itens da lista de medicamentos\n",
        "    medicamento_items = soup.find_all('div', class_='info')  # Ajuste a classe para corresponder ao HTML do site\n",
        "\n",
        "    data = []\n",
        "    for item in medicamento_items:\n",
        "        name = item.find('h2').get_text(strip=True)\n",
        "        price = item.find('span', class_='price').get_text(strip=True) if item.find('span', class_='price') else 'N/A'\n",
        "        data.append({'Name': name, 'Price': price})\n",
        "\n",
        "    # Converte a lista de dados em um DataFrame pandas\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Salva o DataFrame em um arquivo CSV\n",
        "    df.to_csv('medicamentos.csv', index=False)\n",
        "\n",
        "    print(\"Dados dos medicamentos baixados com sucesso em medicamentos.csv\")\n",
        "else:\n",
        "    print(f\"Falha na requisição. Código de status: {response.status_code}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUAAfSLIG-94",
        "outputId": "36d3afcc-4d55-4dd3-f21f-5b8bd9afda58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Falha na requisição. Código de status: 403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL base do Bulário Eletrônico da Anvisa\n",
        "url = \"https://consultas.anvisa.gov.br/#/bulario/q/?nomeProduto=Dipirona\"\n",
        "\n",
        "# Realiza a requisição GET\n",
        "response = requests.get(url)\n",
        "\n",
        "# Verifica se a requisição foi bem-sucedida\n",
        "if response.status_code == 200:\n",
        "    # Analisa o conteúdo HTML com BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    # Aqui você deve identificar a tabela ou o elemento HTML que contém as informações desejadas\n",
        "    # Exemplo: encontrar todas as linhas de uma tabela específica\n",
        "    table = soup.find(\"table\")  # Ajuste conforme a estrutura HTML do site\n",
        "    rows = table.find_all(\"tr\") if table else []\n",
        "\n",
        "    data = []\n",
        "    for row in rows:\n",
        "        cols = row.find_all(\"td\")\n",
        "        cols = [ele.text.strip() for ele in cols]\n",
        "        data.append(cols)\n",
        "\n",
        "    # Converte a lista em um DataFrame pandas\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Salva os dados em um arquivo CSV\n",
        "    df.to_csv('bulario_anvisa.csv', index=False)\n",
        "\n",
        "    print(\"Dados do Bulário baixados com sucesso em bulario_anvisa.csv\")\n",
        "else:\n",
        "    print(f\"Falha na requisição. Código de status: {response.status_code}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmM3PcwIJlgE",
        "outputId": "eb5f185d-a406-43a3-c47a-1b2c40302ae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dados do Bulário baixados com sucesso em bulario_anvisa.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# URL base do Bulário Eletrônico da ANVISA\n",
        "base_url = \"https://dados.gov.br/dados/conjuntos-dados/medicamentos-registrados-no-brasil\"\n",
        "\n",
        "# Parâmetros de pesquisa para o scraping\n",
        "params = {\n",
        "    \"nomeProduto\": \"Paracetamol\",  # Exemplo de nome de medicamento\n",
        "    \"codigo\": \"\"\n",
        "}\n",
        "\n",
        "# Realizando a requisição HTTP para obter a página com as bulas\n",
        "response = requests.get(base_url, params=params)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Extraindo links para bulas\n",
        "bulas = soup.find_all(\"a\", href=True)\n",
        "\n",
        "# Criando arquivo CSV\n",
        "with open('bulas.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Nome do Medicamento\", \"Link para a Bula\"])\n",
        "\n",
        "    for bula in bulas:\n",
        "        link = bula['href']\n",
        "        nome_medicamento = bula.get_text(strip=True)\n",
        "        writer.writerow([nome_medicamento, link])\n",
        "\n",
        "print(\"Dados salvos em bulas.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsPn-mD-Kp2-",
        "outputId": "7b63d207-8fbf-4214-f047-ec6ee2a16f08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dados salvos em bulas.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fastapi uvicorn requests pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5pTrHMoF7Sj",
        "outputId": "71ea244a-c243-42d4-b62a-cf1c1cc64d23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.112.2-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi)\n",
            "  Downloading starlette-0.38.4-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.39.0,>=0.37.2->fastapi) (3.7.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi) (1.2.2)\n",
            "Downloading fastapi-0.112.2-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.38.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, uvicorn, starlette, fastapi\n",
            "Successfully installed fastapi-0.112.2 h11-0.14.0 starlette-0.38.4 uvicorn-0.30.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# main.py\n",
        "from fastapi import FastAPI, HTTPException\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# URL do dataset\n",
        "DATASET_URL = \"https://dados.gov.br/dados/conjuntos-dados/medicamentos-registrados-no-brasil\"\n",
        "CSV_FILE_PATH = \"medicamentos_registrados.csv\"\n",
        "\n",
        "@app.get(\"/\")\n",
        "def read_root():\n",
        "    return {\"message\": \"API para acessar dados de medicamentos registrados no Brasil\"}\n",
        "\n",
        "@app.get(\"/medicamentos/\")\n",
        "def get_medicamentos(limit: int = 10):\n",
        "    try:\n",
        "        # Baixa o dataset e salva em um arquivo CSV\n",
        "        response = requests.get(DATASET_URL)\n",
        "        response.raise_for_status()  # Checa por erros na requisição\n",
        "\n",
        "        # Salva o conteúdo no arquivo CSV\n",
        "        with open(CSV_FILE_PATH, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        # Carrega o CSV para o pandas\n",
        "        data = pd.read_csv(CSV_FILE_PATH, sep=\";\", encoding=\"latin1\")\n",
        "\n",
        "        # Retorna os primeiros 'limit' registros\n",
        "        return data.head(limit).to_dict(orient=\"records\")\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))"
      ],
      "metadata": {
        "id": "V_5ouL2pGQcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WsUqzbQgGeOH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}